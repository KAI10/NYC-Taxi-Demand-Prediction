{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from utils import load_data, accuracy\n",
    "from gat_models import GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "adj, features, labels, idx_train, idx_val, idx_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GAT(\n",
    "    nfeat=features.shape[1], \n",
    "    nhid=8, \n",
    "    nclass=int(labels.max()) + 1, \n",
    "    dropout=0.6, \n",
    "    nheads=8, \n",
    "    alpha=0.2\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(), \n",
    "    lr=0.005, \n",
    "    weight_decay=5e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda()\n",
    "features = features.cuda()\n",
    "adj = adj.cuda()\n",
    "labels = labels.cuda()\n",
    "idx_train = idx_train.cuda()\n",
    "idx_val = idx_val.cuda()\n",
    "idx_test = idx_test.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, adj, labels = Variable(features), Variable(adj), Variable(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastmode = False\n",
    "\n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if not fastmode:\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "        model.eval()\n",
    "        output = model(features, adj)\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.data.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.data.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.data.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.data.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "    return loss_val.data.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.data),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "def train_model(number_of_epochs):\n",
    "    t_total = time.time()\n",
    "    patience = 10\n",
    "    loss_values = []\n",
    "    bad_counter = 0\n",
    "    best = number_of_epochs + 1\n",
    "    best_epoch = 0\n",
    "    for epoch in range(number_of_epochs):\n",
    "        loss_values.append(train(epoch))\n",
    "\n",
    "        torch.save(model.state_dict(), '{}.pkl'.format(epoch))\n",
    "        if loss_values[-1] < best:\n",
    "            best = loss_values[-1]\n",
    "            best_epoch = epoch\n",
    "            bad_counter = 0\n",
    "        else:\n",
    "            bad_counter += 1\n",
    "\n",
    "        if bad_counter == patience:\n",
    "            break\n",
    "\n",
    "        files = glob.glob('*.pkl')\n",
    "        for file in files:\n",
    "            epoch_nb = int(file.split('.')[0])\n",
    "            if epoch_nb < best_epoch:\n",
    "                os.remove(file)\n",
    "\n",
    "    files = glob.glob('*.pkl')\n",
    "    for file in files:\n",
    "        epoch_nb = int(file.split('.')[0])\n",
    "        if epoch_nb > best_epoch:\n",
    "            os.remove(file)\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "    return best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 0.9962 acc_train: 0.7500 loss_val: 1.0150 acc_val: 0.8300 time: 0.6818s\n",
      "Epoch: 0002 loss_train: 0.9584 acc_train: 0.7714 loss_val: 1.0100 acc_val: 0.8333 time: 0.6727s\n",
      "Epoch: 0003 loss_train: 0.9578 acc_train: 0.7643 loss_val: 1.0050 acc_val: 0.8300 time: 0.6741s\n",
      "Epoch: 0004 loss_train: 0.9613 acc_train: 0.7643 loss_val: 1.0001 acc_val: 0.8300 time: 0.6735s\n",
      "Epoch: 0005 loss_train: 1.0359 acc_train: 0.7214 loss_val: 0.9954 acc_val: 0.8300 time: 0.6749s\n",
      "Epoch: 0006 loss_train: 0.9658 acc_train: 0.7571 loss_val: 0.9906 acc_val: 0.8300 time: 0.6732s\n",
      "Epoch: 0007 loss_train: 0.9397 acc_train: 0.7429 loss_val: 0.9858 acc_val: 0.8300 time: 0.6743s\n",
      "Epoch: 0008 loss_train: 0.9492 acc_train: 0.7429 loss_val: 0.9810 acc_val: 0.8300 time: 0.6738s\n",
      "Epoch: 0009 loss_train: 0.8806 acc_train: 0.8000 loss_val: 0.9764 acc_val: 0.8300 time: 0.6736s\n",
      "Epoch: 0010 loss_train: 0.9341 acc_train: 0.7643 loss_val: 0.9721 acc_val: 0.8300 time: 0.6739s\n",
      "Epoch: 0011 loss_train: 1.0123 acc_train: 0.7143 loss_val: 0.9676 acc_val: 0.8300 time: 0.6750s\n",
      "Epoch: 0012 loss_train: 0.9296 acc_train: 0.7500 loss_val: 0.9631 acc_val: 0.8300 time: 0.6490s\n",
      "Epoch: 0013 loss_train: 0.9148 acc_train: 0.7643 loss_val: 0.9589 acc_val: 0.8300 time: 0.6458s\n",
      "Epoch: 0014 loss_train: 0.8917 acc_train: 0.8143 loss_val: 0.9544 acc_val: 0.8300 time: 0.6479s\n",
      "Epoch: 0015 loss_train: 0.9294 acc_train: 0.7429 loss_val: 0.9500 acc_val: 0.8333 time: 0.6471s\n",
      "Epoch: 0016 loss_train: 0.9422 acc_train: 0.7714 loss_val: 0.9455 acc_val: 0.8300 time: 0.6480s\n",
      "Epoch: 0017 loss_train: 0.9473 acc_train: 0.7500 loss_val: 0.9411 acc_val: 0.8300 time: 0.6485s\n",
      "Epoch: 0018 loss_train: 0.9374 acc_train: 0.7286 loss_val: 0.9368 acc_val: 0.8333 time: 0.6446s\n",
      "Epoch: 0019 loss_train: 1.0082 acc_train: 0.7214 loss_val: 0.9327 acc_val: 0.8333 time: 0.6452s\n",
      "Epoch: 0020 loss_train: 0.9202 acc_train: 0.7500 loss_val: 0.9288 acc_val: 0.8333 time: 0.6460s\n",
      "Epoch: 0021 loss_train: 0.9009 acc_train: 0.7500 loss_val: 0.9249 acc_val: 0.8333 time: 0.6452s\n",
      "Epoch: 0022 loss_train: 0.8508 acc_train: 0.7929 loss_val: 0.9212 acc_val: 0.8367 time: 0.6486s\n",
      "Epoch: 0023 loss_train: 0.8702 acc_train: 0.7929 loss_val: 0.9175 acc_val: 0.8367 time: 0.6453s\n",
      "Epoch: 0024 loss_train: 0.8848 acc_train: 0.7786 loss_val: 0.9138 acc_val: 0.8333 time: 0.6457s\n",
      "Epoch: 0025 loss_train: 0.8565 acc_train: 0.7643 loss_val: 0.9104 acc_val: 0.8367 time: 0.6491s\n",
      "Epoch: 0026 loss_train: 0.9092 acc_train: 0.7429 loss_val: 0.9070 acc_val: 0.8400 time: 0.6460s\n",
      "Epoch: 0027 loss_train: 0.9092 acc_train: 0.7286 loss_val: 0.9036 acc_val: 0.8400 time: 0.6478s\n",
      "Epoch: 0028 loss_train: 0.8815 acc_train: 0.7714 loss_val: 0.9006 acc_val: 0.8433 time: 0.6466s\n",
      "Epoch: 0029 loss_train: 0.8784 acc_train: 0.8143 loss_val: 0.8976 acc_val: 0.8400 time: 0.6473s\n",
      "Epoch: 0030 loss_train: 0.8360 acc_train: 0.7714 loss_val: 0.8947 acc_val: 0.8400 time: 0.6462s\n",
      "Epoch: 0031 loss_train: 0.8948 acc_train: 0.7500 loss_val: 0.8919 acc_val: 0.8400 time: 0.6455s\n",
      "Epoch: 0032 loss_train: 0.8384 acc_train: 0.8071 loss_val: 0.8892 acc_val: 0.8400 time: 0.6451s\n",
      "Epoch: 0033 loss_train: 0.8296 acc_train: 0.7571 loss_val: 0.8864 acc_val: 0.8400 time: 0.6456s\n",
      "Epoch: 0034 loss_train: 0.8470 acc_train: 0.8000 loss_val: 0.8834 acc_val: 0.8400 time: 0.6459s\n",
      "Epoch: 0035 loss_train: 0.9498 acc_train: 0.7214 loss_val: 0.8805 acc_val: 0.8367 time: 0.6475s\n",
      "Epoch: 0036 loss_train: 0.8699 acc_train: 0.7643 loss_val: 0.8777 acc_val: 0.8367 time: 0.6463s\n",
      "Epoch: 0037 loss_train: 0.9052 acc_train: 0.7214 loss_val: 0.8751 acc_val: 0.8400 time: 0.6468s\n",
      "Epoch: 0038 loss_train: 0.8156 acc_train: 0.8071 loss_val: 0.8723 acc_val: 0.8367 time: 0.6429s\n",
      "Epoch: 0039 loss_train: 0.8248 acc_train: 0.7714 loss_val: 0.8696 acc_val: 0.8300 time: 0.6435s\n",
      "Epoch: 0040 loss_train: 0.8261 acc_train: 0.7714 loss_val: 0.8669 acc_val: 0.8300 time: 0.6452s\n",
      "Epoch: 0041 loss_train: 0.7246 acc_train: 0.8429 loss_val: 0.8642 acc_val: 0.8300 time: 0.6450s\n",
      "Epoch: 0042 loss_train: 0.7753 acc_train: 0.8071 loss_val: 0.8614 acc_val: 0.8367 time: 0.6464s\n",
      "Epoch: 0043 loss_train: 0.8503 acc_train: 0.7643 loss_val: 0.8586 acc_val: 0.8367 time: 0.6457s\n",
      "Epoch: 0044 loss_train: 0.8323 acc_train: 0.7571 loss_val: 0.8559 acc_val: 0.8333 time: 0.6457s\n",
      "Epoch: 0045 loss_train: 0.8351 acc_train: 0.7857 loss_val: 0.8533 acc_val: 0.8333 time: 0.6454s\n",
      "Epoch: 0046 loss_train: 0.9064 acc_train: 0.7500 loss_val: 0.8509 acc_val: 0.8333 time: 0.6453s\n",
      "Epoch: 0047 loss_train: 0.7814 acc_train: 0.8000 loss_val: 0.8484 acc_val: 0.8333 time: 0.6469s\n",
      "Epoch: 0048 loss_train: 0.8399 acc_train: 0.8071 loss_val: 0.8460 acc_val: 0.8333 time: 0.6441s\n",
      "Epoch: 0049 loss_train: 0.8489 acc_train: 0.7571 loss_val: 0.8439 acc_val: 0.8333 time: 0.6469s\n",
      "Epoch: 0050 loss_train: 0.8929 acc_train: 0.7357 loss_val: 0.8417 acc_val: 0.8333 time: 0.6468s\n",
      "Epoch: 0051 loss_train: 0.8725 acc_train: 0.7857 loss_val: 0.8395 acc_val: 0.8300 time: 0.6454s\n",
      "Epoch: 0052 loss_train: 0.8194 acc_train: 0.7786 loss_val: 0.8372 acc_val: 0.8300 time: 0.6468s\n",
      "Epoch: 0053 loss_train: 0.7688 acc_train: 0.8000 loss_val: 0.8348 acc_val: 0.8267 time: 0.6464s\n",
      "Epoch: 0054 loss_train: 0.8243 acc_train: 0.7857 loss_val: 0.8324 acc_val: 0.8200 time: 0.6469s\n",
      "Epoch: 0055 loss_train: 0.8343 acc_train: 0.7500 loss_val: 0.8301 acc_val: 0.8200 time: 0.6451s\n",
      "Epoch: 0056 loss_train: 0.7416 acc_train: 0.7929 loss_val: 0.8281 acc_val: 0.8200 time: 0.6472s\n",
      "Epoch: 0057 loss_train: 0.8205 acc_train: 0.7857 loss_val: 0.8258 acc_val: 0.8200 time: 0.6460s\n",
      "Epoch: 0058 loss_train: 0.7281 acc_train: 0.7643 loss_val: 0.8237 acc_val: 0.8200 time: 0.6453s\n",
      "Epoch: 0059 loss_train: 0.8223 acc_train: 0.7786 loss_val: 0.8215 acc_val: 0.8167 time: 0.6480s\n",
      "Epoch: 0060 loss_train: 0.8173 acc_train: 0.7857 loss_val: 0.8192 acc_val: 0.8167 time: 0.6425s\n",
      "Epoch: 0061 loss_train: 0.7840 acc_train: 0.7714 loss_val: 0.8169 acc_val: 0.8167 time: 0.6444s\n",
      "Epoch: 0062 loss_train: 0.8051 acc_train: 0.7571 loss_val: 0.8147 acc_val: 0.8167 time: 0.6466s\n",
      "Epoch: 0063 loss_train: 0.7917 acc_train: 0.7786 loss_val: 0.8126 acc_val: 0.8167 time: 0.6490s\n",
      "Epoch: 0064 loss_train: 0.8020 acc_train: 0.7857 loss_val: 0.8104 acc_val: 0.8167 time: 0.6457s\n",
      "Epoch: 0065 loss_train: 0.6682 acc_train: 0.7929 loss_val: 0.8082 acc_val: 0.8167 time: 0.6464s\n",
      "Epoch: 0066 loss_train: 0.7032 acc_train: 0.8286 loss_val: 0.8061 acc_val: 0.8167 time: 0.6445s\n",
      "Epoch: 0067 loss_train: 0.8458 acc_train: 0.7429 loss_val: 0.8042 acc_val: 0.8167 time: 0.6466s\n",
      "Epoch: 0068 loss_train: 0.6983 acc_train: 0.8286 loss_val: 0.8024 acc_val: 0.8167 time: 0.6461s\n",
      "Epoch: 0069 loss_train: 0.8464 acc_train: 0.7500 loss_val: 0.8009 acc_val: 0.8200 time: 0.6459s\n",
      "Epoch: 0070 loss_train: 0.8405 acc_train: 0.7571 loss_val: 0.7992 acc_val: 0.8200 time: 0.6459s\n",
      "Epoch: 0071 loss_train: 0.8367 acc_train: 0.7714 loss_val: 0.7976 acc_val: 0.8200 time: 0.6435s\n",
      "Epoch: 0072 loss_train: 0.6992 acc_train: 0.8357 loss_val: 0.7961 acc_val: 0.8200 time: 0.6444s\n",
      "Epoch: 0073 loss_train: 0.7943 acc_train: 0.7714 loss_val: 0.7946 acc_val: 0.8200 time: 0.6466s\n",
      "Epoch: 0074 loss_train: 0.7933 acc_train: 0.7786 loss_val: 0.7931 acc_val: 0.8200 time: 0.6476s\n",
      "Epoch: 0075 loss_train: 0.7433 acc_train: 0.7643 loss_val: 0.7919 acc_val: 0.8200 time: 0.6476s\n",
      "Epoch: 0076 loss_train: 0.8683 acc_train: 0.7357 loss_val: 0.7908 acc_val: 0.8233 time: 0.6454s\n",
      "Epoch: 0077 loss_train: 0.7511 acc_train: 0.7500 loss_val: 0.7896 acc_val: 0.8233 time: 0.6466s\n",
      "Epoch: 0078 loss_train: 0.7796 acc_train: 0.8071 loss_val: 0.7884 acc_val: 0.8233 time: 0.6441s\n",
      "Epoch: 0079 loss_train: 0.8288 acc_train: 0.7714 loss_val: 0.7872 acc_val: 0.8233 time: 0.6430s\n",
      "Epoch: 0080 loss_train: 0.7244 acc_train: 0.8214 loss_val: 0.7860 acc_val: 0.8267 time: 0.6486s\n",
      "Epoch: 0081 loss_train: 0.7350 acc_train: 0.8071 loss_val: 0.7848 acc_val: 0.8233 time: 0.6468s\n",
      "Epoch: 0082 loss_train: 0.7148 acc_train: 0.8143 loss_val: 0.7836 acc_val: 0.8233 time: 0.6442s\n",
      "Epoch: 0083 loss_train: 0.8047 acc_train: 0.7857 loss_val: 0.7823 acc_val: 0.8233 time: 0.6426s\n",
      "Epoch: 0084 loss_train: 0.8078 acc_train: 0.7643 loss_val: 0.7806 acc_val: 0.8267 time: 0.6444s\n",
      "Epoch: 0085 loss_train: 0.7853 acc_train: 0.7643 loss_val: 0.7789 acc_val: 0.8267 time: 0.6458s\n",
      "Epoch: 0086 loss_train: 0.7593 acc_train: 0.8000 loss_val: 0.7774 acc_val: 0.8267 time: 0.6451s\n",
      "Epoch: 0087 loss_train: 0.6819 acc_train: 0.8500 loss_val: 0.7758 acc_val: 0.8267 time: 0.6464s\n",
      "Epoch: 0088 loss_train: 0.6820 acc_train: 0.8143 loss_val: 0.7743 acc_val: 0.8267 time: 0.6712s\n",
      "Epoch: 0089 loss_train: 0.7212 acc_train: 0.8000 loss_val: 0.7729 acc_val: 0.8267 time: 0.6740s\n",
      "Epoch: 0090 loss_train: 0.7232 acc_train: 0.7929 loss_val: 0.7715 acc_val: 0.8300 time: 0.6718s\n",
      "Epoch: 0091 loss_train: 0.7493 acc_train: 0.7786 loss_val: 0.7702 acc_val: 0.8333 time: 0.6737s\n",
      "Epoch: 0092 loss_train: 0.6934 acc_train: 0.8143 loss_val: 0.7691 acc_val: 0.8333 time: 0.6743s\n",
      "Epoch: 0093 loss_train: 0.8153 acc_train: 0.7786 loss_val: 0.7679 acc_val: 0.8333 time: 0.6720s\n",
      "Epoch: 0094 loss_train: 0.7220 acc_train: 0.8143 loss_val: 0.7669 acc_val: 0.8333 time: 0.6704s\n",
      "Epoch: 0095 loss_train: 0.7413 acc_train: 0.7571 loss_val: 0.7661 acc_val: 0.8333 time: 0.6711s\n",
      "Epoch: 0096 loss_train: 0.7587 acc_train: 0.8071 loss_val: 0.7654 acc_val: 0.8300 time: 0.6738s\n",
      "Epoch: 0097 loss_train: 0.8000 acc_train: 0.7786 loss_val: 0.7647 acc_val: 0.8300 time: 0.6723s\n",
      "Epoch: 0098 loss_train: 0.7965 acc_train: 0.7929 loss_val: 0.7639 acc_val: 0.8300 time: 0.6713s\n",
      "Epoch: 0099 loss_train: 0.7854 acc_train: 0.7857 loss_val: 0.7630 acc_val: 0.8300 time: 0.6722s\n",
      "Epoch: 0100 loss_train: 0.7956 acc_train: 0.7714 loss_val: 0.7621 acc_val: 0.8267 time: 0.6739s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 72.3887s\n"
     ]
    }
   ],
   "source": [
    "best_epoch = train_model(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 99th epoch\n",
      "Test set results: loss= 0.8069 accuracy= 0.8350\n"
     ]
    }
   ],
   "source": [
    "# Restore best model\n",
    "print('Loading {}th epoch'.format(best_epoch))\n",
    "model.load_state_dict(torch.load('{}.pkl'.format(best_epoch)))\n",
    "\n",
    "# Testing\n",
    "compute_test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 1.4.0 Py3.7",
   "language": "python",
   "name": "pytorch140_py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
